# RAGbot â€” Conversations with *Crime and Punishment*

> ðŸš€ **Live App**: [Click here to try RAGbot on Streamlit](https://hsjoi0214-ragbot-appstreamlit-app-xxc6lx.streamlit.app/)

---

## Objective

The goal of this project is to provide an **interactive Retrieval-Augmented Generation (RAG) chatbot** that allows users to explore the novel *Crime and Punishment* by Fyodor Dostoevsky in a conversational manner.  
By combining **document retrieval** with **large language model generation**, RAGbot delivers contextually accurate, memory-aware responses to literary and philosophical questions about the text.

---

## Project Strategy

This app follows a **retrieval + generation** architecture using **LlamaIndex**, **HuggingFace embeddings**, and **Groqâ€™s LLaMA 3 model**.

### Workflow Overview
1. **Document Loading**  
   The full text of *Crime and Punishment* (plaintext file) is ingested using `SimpleDirectoryReader`.
   
2. **Embedding & Indexing**  
   - Uses **sentence-transformers/all-MiniLM-L6-v2** for text embeddings.  
   - Indexed into a vector store for fast semantic search.
   
3. **Context Retrieval**  
   - Retrieves the top-*k* most relevant passages for each query.  
   - `top_k` is configurable in the UI.

4. **Generation with Context**  
   - Groq's **LLaMA 3.3 70B Versatile** model is used for answer generation.  
   - Responses are grounded in retrieved context to reduce hallucination.

5. **Memory-Aware Conversations**  
   - Maintains a buffer of conversation history so the chatbot can respond coherently over multiple turns.

---

## Why This Approach Works

| Component | Purpose | Benefit |
|-----------|---------|---------|
| **HuggingFace Embeddings** | Encode text into vector space | Enables accurate semantic search |
| **VectorStoreIndex** | Store embeddings for fast retrieval | Low-latency, scalable context retrieval |
| **Groq LLaMA 3.3 70B** | Generate answers from context | High-quality, human-like responses |
| **ChatMemoryBuffer** | Store chat history | Provides conversational continuity |
| **Streamlit UI** | Easy web interface | Quick deployment & interaction |

---

## Tools & Libraries

- **Languages**: Python
- **Frameworks**: Streamlit, LlamaIndex
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2
- **LLM Provider**: Groq (LLaMA 3.3 70B Versatile)
- **Others**: python-dotenv for secrets, pathlib for file handling

---

## Key Features

- **Semantic Search** â€” Retrieves the most relevant text excerpts from *Crime and Punishment*.
- **Memory-Aware Chat** â€” Keeps track of past exchanges for contextually coherent conversations.
- **Adjustable Context Depth** â€” `top_k` slider to control how many passages to retrieve.
- **Streamlit UI** â€” Simple, elegant web app interface.
- **Configurable API Keys** â€” Supports `.env` or `.streamlit/secrets.toml`.

---

## Screenshots

### 1. Main Chat Interface
![Chat Interface](assets/mainApp_page.png)

This is the primary interface where users interact with the RAGbot.  
You can ask questions about *Crime and Punishment* and receive context-grounded answers generated by Groqâ€™s LLaMA 3 model.  
A slider allows you to adjust how many relevant text passages (`top_k`) are retrieved per query.

---

### 2. Observability Dashboard
![Observability Dashboard](assets/observability_page_a.png)

The observability dashboard provides detailed timing metrics for each request:
- **Find passages** â€” time to retrieve relevant chunks.  
- **Write answer** â€” time for the LLM to generate the response.  
- **Total roundtrip** â€” end-to-end time from question to answer.  

This view helps identify bottlenecks and monitor efficiency.

---

### 3. Performance Trace Graph
![Performance Trace Graph](assets/observability_page_b.png)

This chart visualizes request timings over multiple queries.  
It highlights spikes in latency (e.g., long generation times) and makes it easy to compare **retrieval**, **generation**, and **roundtrip** performance across sessions.

---

### 4. Monitoring Dashboard
![Monitoring Dashboard](assets/monitoring_page.png)

The monitoring dashboard gives a high-level system health overview:
- **Status** â€” overall health of the system.  
- **Performance Metrics** â€” success rate, throughput, and latency (p95 / p99).  

This ensures the chatbot is running reliably and performing at scale.

---

## Architecture Diagram

![RAGbot Architecture](assets/ragbot_architecture.png)

**Architecture Steps:**
1. **User Query** â†’ Enters prompt in Streamlit chat UI.  
2. **Retriever** â†’ Queries vector store for top-*k* relevant passages.  
3. **LLM** â†’ Groq LLaMA 3.3 70B processes query + retrieved context.  
4. **Response** â†’ Sent back to Streamlit UI and added to memory buffer.  
5. **Conversation History** â†’ Maintains context for multi-turn dialogue.  

---

## Project Structure

```text
ragbot_crime_and_punishment/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ crime_and_punishment.txt   # The full text of Crime and Punishment
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ vector_index/              # Persistent vector index data
â”‚
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml               # Optional API keys for deployment
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ streamlit_app.py           # Main Streamlit application that runs the RAGbot
â”‚   â”œâ”€â”€ config.py                  # Configuration settings (API keys, paths, etc.)
â”‚   â”œâ”€â”€ metrics.py                 # Metrics for monitoring and observability
â”‚   â”œâ”€â”€ tracing.py                 # Trace recording for performance and observability
â”‚   â”œâ”€â”€ feedback.py                # (Optional) Feedback collection module
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env                           # Local development secrets
â”œâ”€â”€ README.md                      # Project description (this file)
```

## Installation & Usage

### Clone this Repository
```bash
git clone https://github.com/hsjoi1402/ragbot-crime-and-punishment.git
cd ragbot-crime-and-punishment
```

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Configure API Key
Set your Groq API key in .env:
```bash
GROQ_API_KEY=your_api_key_here
```

### Run the App
```bash
streamlit run app.py
```
The app will open in your browser at:
http://localhost:8xxx

---

## How It Works (Step-by-Step)
1. Load the novel text from /data/crime_and_punishment.txt.
2. Embed & Index: Create a vector index using HuggingFace embeddings.
3. Persist Index: Store it in /storage/vector_index for reuse.
4. Retrieve Context: On user queries, fetch top-k relevant passages.
5. Generate Answer: Send the context to Groq's LLaMA 3.3 model.
6. Display & Store: Show answer in chat UI and add to conversation history.

---

## Example Queries
1. What is Raskolnikovâ€™s moral struggle?
2. Summarize the conversation between Raskolnikov and Sonia.
3. How does Dostoevsky portray guilt in the novel?

---

## Observability & Monitoring
1. Observability Dashboard â†’ Request timings (retrieval, generation, roundtrip).
2. Performance Graphs â†’ Latency breakdowns across recent queries.
3. Monitoring Dashboard â†’ Success rate, throughput, and system health.

These dashboards make it easy to debug latency spikes, track throughput, and ensure reliability.

---

## Deployment
The app is Streamlit-ready and can be deployed:
1. Locally (via streamlit run)
2. On Streamlit Cloud with .streamlit/secrets.toml
3. In a Docker container for production

---

## Contribution Guidelines
Pull requests are welcome!
Future improvements:
1. Add multi-document support.
2. Enhance UI with richer formatting.
3. Integrate summarization features.
4. Retrieve links and sources with answers.

---

## Author
Prakash Joshi

---

## Acknowledgements
1. **Fyodor Dostoevsky** â€” For writing Crime and Punishment.
2. **ChatGPT (OpenAI)** â€” For providing boilerplate code, improving scripts, and assisting with comments, docstrings, and documentation.